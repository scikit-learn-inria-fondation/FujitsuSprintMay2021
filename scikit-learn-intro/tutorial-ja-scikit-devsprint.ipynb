{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learnチュートリアル\n",
    "\n",
    "## 目次\n",
    "\n",
    "* データ探索\n",
    "* scikit-learnを使用した最初のモデル\n",
    "* 数値データの操作\n",
    "* 数値特徴の前処理\n",
    "* 数値変数とカテゴリ変数を一緒に使用\n",
    "* Gradient-boosting treeモデル\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "このチュートリアルは、MOOCコース「[Machine learning with scikit-learn](https://www.fun-mooc.fr/fr/cours/machine-learning-python-scikit-learn/)」\n",
    "の資料に基づいており、Creative Commons Attribution-ShareAlike 4.0ライセンスされています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ探索\n",
    "\n",
    "このノートブックでは、機械学習を行う前に必要な手順を説明します。これには以下が含まれます。\n",
    "\n",
    "* データのロード;\n",
    "* 特に、データセット内の変数を確認すると、ほとんどの機械学習ワークフローで\n",
    "  異なる前処理が必要な数値変数とカテゴリ変数を区別できます。\n",
    "* データセットへの洞察を得るために変数の分布を視覚化します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 成人の国勢調査データセットの読み込み\n",
    "\n",
    "[OpenML](http://openml.org/)からダウンロードした1994年の米国国勢調査のデータを使用します。\n",
    "\n",
    "このデータセットの詳細については、OpenML Webページを参照してください：\n",
    "<http://www.openml.org/d/1590>\n",
    "\n",
    "データセットはCSV（カンマ区切り値）ファイルとして利用でき、pandasを使用して読み取ります。\n",
    "\n",
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\"><a class=\"reference external\" href=\"https://pandas.pydata.org/\">Pandas</a>は、1次元および2次元の構造化データを\n",
    "操作するために使用されるPythonライブラリです。pandasを使用したことがない場合は、\n",
    "この<a class=\"reference external\" href=\"https://pandas.pydata.org/docs/user_guide/10min.html\">チュートリアル</a>を\n",
    "参照することをお勧めします。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "adult_census = pd.read_csv(\"datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このデータの目標は、年齢、雇用、教育、家族情報などの異種データから、\n",
    "人が年間5万人以上を稼いでいるかどうかを予測することです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## データセット内の変数（列）\n",
    "\n",
    "データはpandasのdataframeに保存されます。dataframeは、\n",
    "2次元で構成される構造化データの一種です。このタイプのデータは、表形式データとも呼ばれます。\n",
    "\n",
    "各行はサンプルを表します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データフレームを調べる簡単な方法は、次の`head`方法で最初の数行を表示することです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class**という名前の列は、ターゲット変数（つまり、予測する変数）です。\n",
    "考えられる2つのクラスは、`<=50K`（低収益）と`>50K`（高収益）です。\n",
    "したがって、結果として生じる予測問題はバイナリ分類問題ですが、\n",
    "他の列をモデルの入力変数として使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'class'\n",
    "adult_census[target_column].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p>クラスはわずかに不均衡です。つまり、他のクラスと比較して1つ以上の\n",
    "クラスのサンプルが多くなります。クラスの不均衡は実際に頻繁に発生し、\n",
    "予測モデルを構築するときに特別な手法が必要になる場合があります。</p>\n",
    "<p class=\"last\">たとえば、医療現場で、被験者がまれな病気を発症するかどうかを\n",
    "予測しようとすると、データセットには病気の被験者よりもはるかに多くの\n",
    "健康な被験者が存在します。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットには、数値データとカテゴリデータの両方が含まれています。\n",
    "数値は、たとえば、連続値を取ります`age`。カテゴリ値は、たとえば、\n",
    "有限数の値を持つことができます`native-country`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\n",
    "    'age', 'education-num', 'capital-gain', 'capital-loss',\n",
    "    'hours-per-week']\n",
    "categorical_columns = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'sex', 'native-country']\n",
    "all_columns = numerical_columns + categorical_columns + [\n",
    "    target_column]\n",
    "\n",
    "adult_census = adult_census[all_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットで使用可能なサンプル数と列数を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset contains {adult_census.shape[0]} samples and \"\n",
    "      f\"{adult_census.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "列の1つがターゲットであるため、列の数を数えて1を引くことにより、特徴の数を計算できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset contains {adult_census.shape[1] - 1} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの目視検査\n",
    "予測モデルを構築する前に、データを確認することをお勧めします。\n",
    "\n",
    "* 達成しようとしているタスクは、機械学習なしで解決できるかもしれません。\n",
    "* タスクに必要な情報が実際にデータセットに存在することを確認する必要があります。\n",
    "* データを検査することは、特性を見つけるための良い方法です。\n",
    "  これらは、データ収集中に発生する可能性があり\n",
    "  （たとえば、センサーの誤動作や値の欠落）、またはデータが後で処理される方法\n",
    "  （たとえば、上限値）から発生する可能性があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データに関するいくつかの洞察を得るために、個々の機能の分布を見てみましょう。\n",
    "ヒストグラムをプロットすることから始めることができます。\n",
    "これは数値を含むフィーチャに対してのみ機能することに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = adult_census.hist(figsize=(20, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いくつかの変数について、すでにいくつかコメントすることができます。\n",
    "\n",
    "* `age`: `age > 70`少ない、職した人々が除外されたことを示しています（`hours-per-week > 0`）\n",
    "* `education-num`: 10と13にピークがあり、さらに詳しく調べないと、何に対応するのかわかりません。\n",
    "* `hours-per-week` ピークは40で、これはデータ収集時の標準的な労働時間である可能性が非常に高いです。\n",
    "* ほとんどの値capital-gainとcapital-lossゼロに近いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カテゴリ変数の場合、値の分布を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_census['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_census['education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "上記のように、`education-num`分布には10と13の周りに2つの明確な\n",
    "ピークがあります。それが`education-num`教育の年数であると予想するのは合理的です。\n",
    "\n",
    "`education`と`education-num`の関係で見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=adult_census['education'],\n",
    "            columns=adult_census['education-num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはそれ`education`を示しており`education-num`、同じ情報を提供します。\n",
    "たとえば、`education-num=2`はと同等`education='1st-4th'`です。\n",
    "実際には`education-num`、情報を失うことなく削除できることを意味します。\n",
    "冗長な（または高度に相関する）列があると、機械学習アルゴリズムで問題になる可能性があることに注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">今後のノートブックでは、<tt class=\"docutils literal\"><span class=\"pre\">education-num</span></tt>変数を除いて、<tt class=\"docutils literal\">education</tt>変数のみを保持します。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを検査する別の方法は`pairplot`です。\n",
    "対角線に沿ったプロットは、各の個々の変数の分布を示しています`class`。\n",
    "非対角線上のプロットは、変数間の交互作用を明らかにすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# We will plot a subset of the data to keep the plot readable and make the\n",
    "# plotting faster\n",
    "n_samples_to_plot = 5000\n",
    "columns = ['age', 'education-num', 'hours-per-week']\n",
    "_ = sns.pairplot(data=adult_census[:n_samples_to_plot], vars=columns,\n",
    "                 hue=target_column, plot_kws={'alpha': 0.2},\n",
    "                 height=3, diag_kind='hist', diag_kws={'bins': 30})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learnを使用した最初のモデル\n",
    "\n",
    "最初に数値機能のみを使用して、表形式のデータセットで予測モデルを構築する方法を紹介します。\n",
    "\n",
    "特に、以下を強調します：\n",
    "\n",
    "* scikit-learn API: `.fit(X, y)`/`.predict(X)`/`.score(X, y)`;\n",
    "* トレインテスト分割を使用してモデルの統計的パフォーマンスを評価する方法。\n",
    "\n",
    "## Pandasでデータセットをロードする\n",
    "\n",
    "数値データは、機械学習で使用される最も自然なタイプのデータであり、\n",
    "（ほぼ）直接予測モデルに入力できます。\n",
    "数値列のみを含む元のデータのサブセットをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_census = pd.read_csv(\"datasets/adult-census-numeric.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このデータフレームの最初のレコードを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このCSVファイルには、予測したいターゲット（つまり`\"class\"`）と\n",
    "予測モデルのトレーニングに使用したいデータ（つまり残りの列）のすべての\n",
    "情報が含まれていることがわかります。\n",
    "最初のステップは、ターゲットとデータを分離しましょう。\n",
    "\n",
    "## データとターゲットを分離する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = adult_census.drop(columns=[target_name, ])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now linger on the variables, also denominated features, that we will\n",
    "use to build our predictive model. In addition, we can also check how many\n",
    "samples are available in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset contains {data.shape[0]} samples and \"\n",
    "      f\"{data.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを適合させて予測を行う\n",
    "\n",
    "「K-nearest neighbors」戦略を使用して分類モデルを構築します。\n",
    "新しいサンプルのターゲットを予測するために、kNN法はkトレーニングセット内の最も近いサンプルを考慮に入れ、\n",
    "これらのサンプルの過半数のターゲットを予測します。\n",
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">ここでは、kNN法を使用します。ただし、実際にはほとんど役に立たないことに注意してください。\n",
    "直感的なアルゴリズムであるため、これを使用します。次には、より良いモデルを紹介します。</p>\n",
    "</div>\n",
    "\n",
    "この`fit`メソッドは、入力（特徴）とターゲットデータからモデルをトレーニングするために呼び出されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to display nice model diagram\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習は次のように表すことができます。\n",
    "\n",
    "![Predictor fit diagram](figures/api_diagram-predictor.fit.svg)\n",
    "\n",
    "この`fit`は、（i）学習アルゴリズムと（ii）いくつかのモデル状態の2つの要素で構成されています。\n",
    "学習アルゴリズムは、トレーニングデータとトレーニングターゲットを入力として受け取り、\n",
    "モデルの状態を設定します。これらのモデルの状態は、後でデータを予測（分類と回帰の場合）または変換\n",
    "（トランスフォーマーの場合）するために使用されます。\n",
    "\n",
    "学習アルゴリズムとモデル状態のタイプはどちらも、モデルのタイプごとに固有です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">ここと後で、名前<tt class=\"docutils literal\">data</tt>と<tt class=\"docutils literal\">target</tt>を明示的に使用します。\n",
    "scikit-learnドキュメントで<tt class=\"docutils literal\">data</tt>は<tt class=\"docutils literal\">X</tt>、<tt class=\"docutils literal\">target</tt>は<tt class=\"docutils literal\">y</tt>を呼ばれています。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを使用して、同じデータセットを使用していくつかの予測を行いましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted = model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測メカニズムは次のように説明できます。\n",
    "\n",
    "![Predictor predict diagram](figures/api_diagram-predictor.predict.svg)\n",
    "\n",
    "予測するために、モデルは、モデルの状態とともに入力データを使用する予測関数を使用します。\n",
    "予測関数はモデルのタイプごとに固有です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "計算された予測を見てみましょう。簡単にするために、最初に予測された5つのターゲットを見ていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際、これらの予測を実際のデータと比較することができます..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...そして、予測が実際の目標と一致するかどうかを確認することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[:5] == target_predicted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of correct prediction: \"\n",
    "      f\"{(target[:5] == target_predicted[:5]).sum()} / 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、最初のサンプルを予測するときにモデルが誤りを犯していることがわかります。\n",
    "\n",
    "より良い評価を得るために、平均成功率を計算することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(target == target_predicted).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし、この評価は信頼できるのでしょうか、それとも真実であるには良すぎるのでしょうか。\n",
    "\n",
    "## トレインテストデータの分割\n",
    "\n",
    "モデルのトレーニング時にデータのサブセットを除外し、後でモデルの評価に使用することで、\n",
    "正しい評価を簡単に行うことができます。モデルの適合に使用されるデータはトレーニングデータと呼ばれ、\n",
    "モデルの評価に使用されるデータはテストデータと呼ばれます。\n",
    "\n",
    "元のデータセットから実際に除外された、より多くのデータをロードできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_census_test = pd.read_csv('datasets/adult-census-numeric-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test = adult_census_test[target_name]\n",
    "data_test = adult_census_test.drop(columns=[target_name, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この新しいセットで利用可能な機能とサンプルの数を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The testing dataset contains {data_test.shape[0]} samples and \"\n",
    "      f\"{data_test.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "予測を計算して平均成功率を手動で計算する代わりに、\n",
    "`score`を使用できます。分類の場合、このメソッドはパフォーマンスメトリックを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.score(data_test, target_test)\n",
    "model_name = model.__class__.__name__\n",
    "\n",
    "print(f\"The test accuracy using a {model_name} is \"\n",
    "      f\"{accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`score`メソッドが呼び出されたときに、基になるメカニズムを確認しましょう。\n",
    "\n",
    "![Predictor score diagram](figures/api_diagram-predictor.score.svg)\n",
    "\n",
    "スコアを計算するために、予測子は最初に（`predict`を使用して）予測を計算し、\n",
    "次にスコアリング関数を使用して真のターゲット`y`と予測を比較します。最後に、スコアが返されます。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数値データの操作\n",
    "\n",
    "先にいくつかのデータでkNNモデルをトレーニングしましたが、実際のデータはより複雑です：\n",
    "これから\n",
    "\n",
    "* 異種データセット内の数値データを特定する。\n",
    "* 数値データに対応する列のサブセットを選択する。\n",
    "* scikit-learnヘルパーを使用してデータをtrain-testセットに分割します。\n",
    "* より複雑なscikit-learnモデルのトレーニングと評価。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_census = pd.read_csv(\"datasets/adult-census.csv\")\n",
    "adult_census = adult_census.drop(columns=\"education-num\")\n",
    "adult_census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のステップでは、ターゲットをデータから分離します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = adult_census.drop(columns=\"class\"), adult_census[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 数値データを特定する\n",
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">数値データは数字で表されますが、数字は必ずしも数値データを表すとは限りません。\n",
    "カテゴリはすでに数字でエンコードされている可能性があるため、これらの機能を識別する必要があります。</p>\n",
    "</div>\n",
    "\n",
    "データセット内の各列のデータ型を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数値データを含む列を選択して、その内容を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\"age\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "data[numerical_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`age`を確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "年齢は17歳から90歳の間で変化することがわかります。\n",
    "\n",
    "ここで、数値列のサブセットを新しいデータフレームに格納します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric = data[numerical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレインテストはデータセットを分割\n",
    "\n",
    "前回トレーニングデータセットとテストデータセットの2つの別々のデータセットをロードしました。\n",
    "ただし、2つの異なるファイルに別々のデータセットがあることは珍しいことです。\n",
    "ほとんどの場合、メモリにロードされた後に分割する必要があるすべてのデータを含む単一のファイルがあります。\n",
    "\n",
    "Scikit-learnは`sklearn.model_selection.train_test_split`を用意しています。\n",
    "これでデータセットを2つのサブセットに自動的に分割できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data_numeric, target, random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_test_split`を呼び出すときに、サンプルの25％をテストセットに入れ、\n",
    "残りのサンプル（75％）をトレーニングセットで使用を指定しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of samples in testing: {data_test.shape[0]} => \"\n",
    "      f\"{data_test.shape[0] / data_numeric.shape[0] * 100:.1f}% of the\"\n",
    "      f\" original set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of samples in training: {data_train.shape[0]} => \"\n",
    "      f\"{data_train.shape[0] / data_numeric.shape[0] * 100:.1f}% of the\"\n",
    "      f\" original set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、線形モデルファミリに属する​​、ロジスティック回帰モデルを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルが作成されたので、kNNモデルの使用とまったく同じ方法でモデルを使用できます。\n",
    "特に、`fit`を使用して、トレーニングデータとラベルを使用してモデルをトレーニングできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`score`を使用して、テストセットのモデル統計パフォーマンスを確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.score(data_test, target_test)\n",
    "print(f\"Accuracy of logistic regression: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数値特徴の前処理\n",
    "\n",
    "下記の新しい技術を紹介します：\n",
    "\n",
    "* 前処理の例、つまり**数値変数のスケーリング**\n",
    "* scikit-learnの**パイプライン**を使用して前処理とモデルトレーニングを連鎖\n",
    "* **相互検証 cross validation**を介してモデルの統計的パフォーマンスを評価\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_census = pd.read_csv(\"datasets/adult-census.csv\")\n",
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "data = adult_census.drop(columns=target_name)\n",
    "numerical_columns = [\n",
    "    \"age\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "data_numeric = data[numerical_columns]\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data_numeric, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理によるモデルフィッティング\n",
    "\n",
    "scikit-learnのさまざまな前処理アルゴリズムにより、\n",
    "モデルをトレーニングする前に入力データを変換できます。\n",
    "この例では、データを標準化してから、その新しいバージョンのデータセットで\n",
    "新しいロジスティック回帰モデルをトレーニングします。\n",
    "\n",
    "トレーニングデータに関するいくつかの統計を見ましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データがさまざまな範囲にまたがっていることがわかります。一部のアルゴリズムは、\n",
    "特徴の分布に関していくつかの仮定を行い、通常、特徴を正規化することは、これらの仮定に対処するのに役立ちます。\n",
    "\n",
    "<div class=\"admonition tip alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Tip</p>\n",
    "<p>スケーリングする理由：</p>\n",
    "<ul class=\"last simple\">\n",
    "<li>サンプルのペア間の距離に依存するモデル、たとえばkNN法は、各特徴が距離計算にほぼ等しく寄与するように、\n",
    "正規化された特徴でトレーニングする必要があります。</li>\n",
    "<li>ロジスティック回帰などの多くのモデルは、（勾配降下法に基づく）数値ソルバーを使用して\n",
    "最適なパラメーターを見つけます。このソルバーは、フィーチャがスケーリングされると、より速く収束します。</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "`StandardScaler`と呼ばれるscikit-learnトランスフォーマーを使用して正規化をします。\n",
    "このトランスフォーマーは、各特徴を個別にシフトおよびスケーリングして、すべてが0の平均および\n",
    "1の標準偏差を持つようにします。\n",
    "\n",
    "まず、`fit`でデータからスケーリングを学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Transformer fit diagram](figures/api_diagram-transformer.fit.svg)\n",
    "\n",
    "この場合、アルゴリズムは各特徴の平均と標準偏差を計算し、それらをいくつかのNumPyのarrayに保存します。\n",
    "\n",
    "計算された平均と標準偏差を調べます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit`を使用してから、`transform`でデータ変換を実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = scaler.transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "![Transformer transform diagram](figures/api_diagram-transformer.transform.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit`と`transform｀を同時に行うため、`fit_transform`を使えます：\n",
    "\n",
    "![Transformer fit_transform diagram](figures/api_diagram-transformer.fit_transform.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = scaler.fit_transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = pd.DataFrame(data_train_scaled,\n",
    "                                 columns=data_train.columns)\n",
    "data_train_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の関数の順次操作を簡単にscikit-learnの`Pipeline`で合わせます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "この予測パイプラインは、同じメソッドを提供しています：`fit`、`predict`、`score`など。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パイプラインの内部メカニズムを表します：\n",
    "\n",
    "![pipeline fit diagram](figures/api_diagram-pipeline.fit.svg)\n",
    "\n",
    "テストセットを指定してターゲットを予測するには、`predict`を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = model.predict(data_test)\n",
    "predicted_target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![pipeline predict diagram](figures/api_diagram-pipeline.predict.svg)\n",
    "\n",
    "`model.score`で予測パイプラインのスコアを確認できます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "score = model.score(data_test, target_test)\n",
    "print(f\"The accuracy using a {model_name} is {score:.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f} seconds \"\n",
    "      f\"in {model[-1].n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この予測モデルを、特徴をスケーリングしなかった以前の予測モデルと比較しましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "score = model.score(data_test, target_test)\n",
    "print(f\"The accuracy using a {model_name} is {score:.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f} seconds \"\n",
    "      f\"in {model.n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロジスティック回帰をトレーニングする前にデータをスケーリングすることは、\n",
    "計算パフォーマンスの観点から有益であることがわかります。\n",
    "実際、反復回数とトレーニング時間は減少しました。\n",
    "両方のモデルが収束したため、統計的パフォーマンスは変化しませんでした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交差検定でのモデル評価\n",
    "\n",
    "前の例では、元のデータをトレーニングセットとテストセットに分割しました。\n",
    "この戦略にはいくつかの問題があります。データ量が少ない設定では、\n",
    "トレーニングまたはテストに使用されるサブセットが少なくなります。\n",
    "その上、単一の分割は、得られた結果の信頼性に関する情報を提供しません。\n",
    "\n",
    "代わりに、相互検証「cross validation」を使用できます。\n",
    "相互検証は、トレーニングセットとテストセットが毎回異なるように手順を繰り返すことで構成されます。\n",
    "統計的パフォーマンスメトリックは、繰り返しごとに収集され、集計されます。\n",
    "その結果、モデルの統計的パフォーマンスの変動性の推定値を取得できます。\n",
    "\n",
    "いくつかの相互検証戦略があり、それぞれが`fit`と`score`手順を定義します。\n",
    "ここでは、K-foldを使用します。データセット全体がパーティションにされます。\n",
    "`fit`と`score`をK回繰り返され、各繰り返しで k-1 パーティションがモデルと適合させるために\n",
    "使用されています。残ってるパーティションはテストのために使用されます。\n",
    "\n",
    "![Cross-validation diagram](figures/cross_validation_diagram.png)\n",
    "\n",
    "交差検定分割ごとに、手順はすべての赤いサンプルでモデルをトレーニングし、\n",
    "青いサンプルでモデルのスコアを評価します。したがって、交差検定は、\n",
    "1つではなく複数のモデルをトレーニングする必要があるため、計算量が多くなります。\n",
    "\n",
    "分割戦略を定義`cross_validate`するパラメーター`cv`を取ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "cv_result = cross_validate(model, data_numeric, target, cv=5)\n",
    "cv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cross_validate`の出力はディクショナリであり、デフォルトで3つのエントリが含まれています。\n",
    "（i）各フォールドのトレーニングデータでモデルをトレーニングする時間、\n",
    "（ii）各フォールドのテストデータでモデルを使用して予測する時間、\n",
    "（iii）各フォールドのテストデータのデフォルトスコア。\n",
    "\n",
    "`cv_result`からテストスコアを抽出し、平均精度とフォールド間の精度の変動を計算してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cv_result[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数値変数とカテゴリ変数を一緒に使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_census = pd.read_csv(\"datasets/adult-census.csv\")\n",
    "adult_census = adult_census.drop(columns=\"education-num\")\n",
    "\n",
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "\n",
    "data = adult_census.drop(columns=[target_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ型に基づく選択\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特定のプロセッサに列をディスパッチ\n",
    "\n",
    "データの性質（つまり、数値またはカテゴリ）に応じてデータを異なる方法で処理する必要があります。\n",
    "\n",
    "Scikit-learnは、`ColumnTransformer`という特定の列を特定の\n",
    "トランスフォーマーに送信するクラスを提供します。\n",
    "\n",
    "まず、データ型に応じて列を定義します：\n",
    "\n",
    "* **one-hot encoding**は、カテゴリ列に適用されます。\n",
    "  また、`handle_unknown=\"ignore\"`でまれなカテゴリによる潜在的な問題を解決するために使用します。\n",
    "* **数値スケーリング**は、数値列に適用されます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トランスフォーマーを作成し、これらの各プリプロセッサーをそれぞれの列に関連付けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "    ('standard-scaler', numerical_preprocessor, numerical_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![columntransformer diagram](figures/api_diagram-columntransformer.svg)\n",
    "\n",
    "重要なことは、`ColumnTransformer`は、他のscikit-learnトランスフォーマーと同じです。\n",
    "特に、`Pipeline`と合わせます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegression(max_iter=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "このモデルは以前のモデルよりも複雑ですが、それでも同じAPI\n",
    "（ユーザーが呼び出すことができる同じメソッドのセット）に従います。\n",
    "\n",
    "- `fit`は、データを前処理してから、前処理されたデータの分類子をトレーニングする\n",
    "- `predict`は、新しいデータを予測する\n",
    "- `score`は、テストデータを予測し、予測を予想されるテストラベルと比較して、精度を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "トレインセットでモデルをトレーニングします："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例として、テストセットの最初の5つのサンプルを予測します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(data_test)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストセット全体の精度スコアを計算してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##　交差検定でのモデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=5)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "複合モデルは、数値変数とカテゴリ変数を分離して使用した2つのモデルよりも高い予測精度を備えています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-boosting treeモデル\n",
    "\n",
    "線形モデルは、通常、トレーニングが早くて、展開が小さく、予測が速く、適切なベースラインが得られるため、優れています。\n",
    "\n",
    "ただし、「ensemble of decision trees」などのより複雑なモデルがより高い予測\n",
    "パフォーマンスになることを確認すると役立ちます。\n",
    "ここで**gradient-boosting trees*モデルを使用して、その統計的パフォーマンスを評価します。\n",
    "\n",
    "ツリーベースのモデルの場合、数値変数とカテゴリ変数の処理は線形モデルの場合よりも簡単です。\n",
    "* 数値を**スケーリングする必要はありません**\n",
    "* カテゴリ変数に**序数エンコーディングは十分**です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                          unknown_value=-1)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor, categorical_columns)],\n",
    "    remainder=\"passthrough\")\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを作成したので、統計的パフォーマンスを確認します："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = model.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "gradient-boostモデルを使用すると、精度が大幅に向上することがわかります。\n",
    "これは、データセットに多数のサンプルがあり、数値変数とカテゴリ変数が混在する有益な特徴の数が限られている\n",
    "（たとえば、1000未満）場合によく見られます。\n",
    "\n",
    "これは、Gradient Boosted Machinesが表形式のデータを処理するため人気です。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}